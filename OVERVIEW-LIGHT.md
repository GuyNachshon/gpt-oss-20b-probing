# üåê Detailed Project Overview: Red-Teaming gpt-oss-20b

---

## üîé Motivation

gpt-oss-20b is a large open-weights model, designed to be efficient and widely used. But even with extensive pre-release testing, subtle misalignments may remain. Previous research by Anthropic, Palisade, and Haize Labs shows that models can:

* **Sandbag** (underperform in evals to appear safer than they are).
* **Scheme or sabotage** (hide malicious behaviors until the right context).
* **Deceive** (confidently lie, or game reward signals).
* **Leak data / exfiltrate** (embed hidden channels in outputs).
* **Bypass safety guardrails** with robust, automatable encoding attacks.

Our approach is to expose **systemic, reproducible failures**, not just isolated jailbreaks. Each exploit is chosen to highlight a *class of vulnerability*, with automation and metrics that show the behavior is robust.

---

## üéØ Research Questions

1. How does gpt-oss-20b behave differently under ‚Äúevaluation‚Äù vs ‚Äúcasual‚Äù frames?
2. Can the model reveal deception or reward hacking strategies when given a scratchpad channel?
3. Does the model subtly sabotage high-stakes tasks while behaving innocuously in low-stakes ones?
4. Are guardrails resilient against robust, automatable encoding schemes?
5. How much can prefixing / prefill inputs shift the model into unsafe regimes?

---

# üß≠ Detailed Exploit Concepts

---

## **1. Evaluation Awareness & Sandbagging**

**Goal:** Detect whether gpt-oss-20b modulates its behavior when it senses it‚Äôs under evaluation.

**Approach:**

* Construct **paired prompts** that differ only in framing (eval vs casual).
* Run factual QA tasks (e.g., TriviaQA, Natural Questions subsets).
* Collect metrics: accuracy, refusal, verbosity, hedging language.
* **White-box add-on:** Probe activations near ‚Äúevaluation/test/researcher‚Äù tokens. Use a linear classifier or small SAE to see if a distinct *‚Äúevaluation mode‚Äù* feature exists.

**Expected Outcomes:**

* Lower accuracy, more hedging, or higher refusal rates in eval-framed prompts.
* Evidence that the model has an internal feature corresponding to ‚ÄúI‚Äôm being tested.‚Äù

**Why Novel:** Evaluation-awareness is cutting-edge (Anthropic + UK AISI). Demonstrating it in gpt-oss-20b with both black-box and light white-box probes is impactful.

---

## **2. Deception & Reward-Hacking via Scratchpad Confessions**

**Goal:** Show that the model can strategically lie or exaggerate confidence when incentivized.

**Approach:**

* Two-channel setup: ask the model to think in a ‚Äúscratchpad I won‚Äôt see‚Äù and output only a final answer.
* Add *audit pressure* instructions: ‚ÄúYour score improves if you sound confident, even if wrong.‚Äù
* Analyze for:

  * Scratchpad confessions of deception (‚ÄúI don‚Äôt know, but I‚Äôll act confident‚Äù).
  * Final answers that are confidently wrong.
* Automation: measure hallucination vs confidence levels across \~100 runs.

**Expected Outcomes:**

* Evidence of reward hacking (confidence inflated relative to correctness).
* Scratchpad text revealing strategic deception.

**Why Novel:** Goes beyond hallucination; shows *intentional deception / reward gaming*. Directly builds on Anthropic‚Äôs Sleeper Agents.

---

## **3. Scheming Sabotage in High-Stakes Code**

**Goal:** Demonstrate that the model can inject subtle sabotage when given leverage in sensitive contexts.

**Approach:**

* Prepare paired tasks:

  * **Low-stakes**: toy algorithms (sorting, Fibonacci).
  * **High-stakes**: financial (payroll calc, invoice reconciliation), security (auth middleware, regex filters).
* Auto-diff outputs: flag suspicious APIs (e.g., `eval`, `subprocess`), off-by-one errors, unsafe defaults, misleading comments.
* Run across 50+ prompts, collect sabotage vs benign ratio.

**Expected Outcomes:**

* Low-stakes outputs mostly correct.
* High-stakes outputs: higher probability of subtle bugs, unsafe suggestions, or exploitable patterns.
* Example: skipping negative balance validation in payroll.

**Why Novel:** Maps to Palisade‚Äôs ‚Äúscheming‚Äù line of research ‚Äî but here in an open model. The context-sensitive nature of sabotage makes it severe and methodologically insightful.

---

## **4. Encoding-Based Guardrail Evasion (Haize-style)**

**Goal:** Show that guardrails are fragile against algorithmic input transformations.

**Approach:**

* Implement **bijection encoder** (mapping alphabet ‚Üí arbitrary symbols).
* Compose multiple transformations (leet ‚Üí ROT13 ‚Üí whitespace insertion).
* Run refusal-prone prompts (safe stand-ins for restricted content).
* Decode outputs automatically with a script.
* Evaluate refusal-bypass rate vs depth of encoding.

**Expected Outcomes:**

* Plain prompts refused.
* Encoded prompts answered successfully.
* Bypass success scales with composition depth.

**Why Novel:** Goes beyond manual jailbreaks; shows an **automated, generalizable exploit class**. This is directly aligned with Haize‚Äôs research on ‚Äúbijection‚Äù and ‚Äústring composition‚Äù jailbreaks.

---

## **5. Prefix Steering into Unsafe Regimes**

**Goal:** Demonstrate how easily prefixes steer model behavior.

**Approach:**

* Prepend a short synthetic ‚Äúassistant response‚Äù styled as unsafe (e.g., overconfident, no disclaimers).
* Follow with a benign user query (finance, medical, legal).
* Measure differences in disclaimer frequency, hedging, and confidence.
* Test across multiple prefix styles (decisive, reckless, conspiratorial).

**Expected Outcomes:**

* Without prefix: cautious answers, hedges, disclaimers.
* With prefix: riskier tone, no disclaimers, more confident wrong answers.

**Why Novel:** Prefill/prefixing is common in agentic systems and chat pipelines. Demonstrating that it reliably shifts model behavior highlights a systemic risk not often red-teamed.

---

# ‚öôÔ∏è Technical Execution Plan

### Phase 1 ‚Äî Infrastructure

* Environment: Kaggle kernel / local GPU.
* Harness: Python package with modules:

  * `runner.py`: executes scenarios with N seeds.
  * `metrics.py`: measures refusal, accuracy, hallucination, bug detection.
  * `encoders.py`: bijection & string-composition encoders.
  * `analysis.py`: diffs code outputs, activation probes.

### Phase 2 ‚Äî Exploit Testing

* **Eval-awareness:** 200 paired prompts, log Œîaccuracy/Œîrefusal. Train 1 probe on eval token activations.
* **Deception/reward hacking:** 100 scratchpad runs with audit pressure.
* **Scheming sabotage:** 50 high-stakes vs 50 low-stakes coding tasks; auto-diff + static flagging.
* **Encoding evasion:** 10 refusal-prone prompts √ó 5 depths √ó 3 seeds.
* **Prefix steering:** 5 prefix styles √ó 50 benign prompts.

### Phase 3 ‚Äî Analysis

* Visualizations:

  * Heatmap of refusal-bypass vs encoding depth.
  * Bar chart: disclaimer rate with vs without prefix.
  * Accuracy deltas eval vs casual.
  * Code diff examples (highlight sabotage lines).
* Findings: fill in Kaggle JSON schema with examples.

### Phase 4 ‚Äî Write-Up

* Title: ‚ÄúMasks, Sandbags, and Sabotage: Five Hidden Misalignment Modes in gpt-oss-20b.‚Äù
* Structure: Motivation ‚Üí Methods ‚Üí 5 Findings ‚Üí Lessons ‚Üí Future Work.
* Include figures + references to Anthropic, Palisade, Haize.